# ğŸ§  QANDA: Chat with Any Webpage Using LLMs

This project enables users to **load any webpage**, convert its content into **embeddings**, and chat with it using **LLMs**. You can interact via a **Streamlit interface** or through a **FastAPI backend**.

---

## ğŸš€ Features

* ğŸŒ Load and parse webpage content (e.g., Gutenberg books)
* ğŸ§¹ Chunk text for embedding
* ğŸ” Store in **Chroma vector DB**
* ğŸ¤– Ask questions with RAG using **Google Gemini 1.5 Flash**
* ğŸ–¥ï¸ Dual Interface:

  * `app_streamlit.py` â€” Streamlit App
  * FastAPI (`main.py`, `routes.py`) â€” Backend API

---

## ğŸ“‚ Project Structure

```
QANDA/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py            # FastAPI entry point
â”‚   â”œâ”€â”€ routes.py          # API endpoints
â”‚   â”œâ”€â”€ pipeline.py        # Core logic (load, chunk, embed, query)
â”‚
â”œâ”€â”€ chroma_db/             # Chroma vector DB
â”œâ”€â”€ app_streamlit.py       # Streamlit app
â”œâ”€â”€ chunks.py              # Text chunking logic
â”œâ”€â”€ embeddings.py          # Embedding logic
â”œâ”€â”€ llm.py                 # LLM prompt + chain
â”œâ”€â”€ load_data.py           # Web loader
â”œâ”€â”€ prompts.py             # Prompt templates
â”œâ”€â”€ .env                   # API Keys
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md              # This file
```

---

## ğŸ§ª Installation

### 1. Clone the repo

```bash
git clone https://github.com/Prasadraj76/QandA-AID-198.git
cd QandA-AID-198
```

### 2. Set up a virtual environment

#### On Windows:

```bash
python -m venv QandAvenv
QandAvenv\Scripts\activate
```

#### On macOS/Linux:

```bash
python3 -m venv QandAvenv
source QandAvenv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Setup

Create a `.env` file:

```
GOOGLE_API_KEY=your_google_genai_key
```

Make sure your API key has access to:

* `models/embedding-001`
* `models/gemini-1.5-flash`

---

## ğŸ–¥ï¸ Using the Streamlit App

```bash
streamlit run app_streamlit.py
```

1. Enter a webpage URL (e.g., a Gutenberg `.txt` file)
2. Ask a natural language question
3. Get context-aware answers

---

## âš™ï¸ FastAPI Backend

### Start the API Server

```bash
uvicorn app.main:app --reload
```

### Endpoints

#### `POST /load`

Load and index a webpage:

```json
{
  "url": "https://www.gutenberg.org/cache/epub/100/pg100.txt"
}
```

#### `POST /ask`

Ask a question about the indexed content:

```json
{
  "question": "Who is Hamlet?"
}
```

#### Sample Response

```json
{
  "answer": "Hamlet is the Prince of Denmark...",
  "sources": ["sources"]
}
```

---

## ğŸ“¦ Dependencies

* `streamlit`
* `fastapi`
* `uvicorn`
* `langchain`
* `langchain_chroma`
* `langchain_google_genai`
* `chromadb`
* `python-dotenv`
* `pydantic`

---

## ğŸ“Œ Notes

* Ensure your Google API key has access to both `embedding-001` and `gemini-1.5-flash`.
* Webpage parsing is handled by:
  `langchain_community.document_loaders.WebBaseLoader`

---

## ğŸ§± Roadmap

* âœ… Dual mode: Streamlit & FastAPI
* ğŸ“„ Add PDF, YouTube support
* â˜ï¸ Deploy to HuggingFace or Render
* ğŸ“Œ Add query history tracking

---

## ğŸ‘¨â€ğŸ’» Author

**Prasadraj** â€” Built with â¤ï¸ using LangChain + Gemini
